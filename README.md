# Spark_Streaming_Pipeline
# Spark-Streaming-Project

Proof Of Concept for Apache Spark ,Apache Kakfa and MongoDB Integration	

I wanted to setup a data ingestion pipeline to process events recieved from Kafka and persist them to repository. This workflow can be used for example when events are generated by telemetics device and shown in website dashboard

Technologies Used :

1) Apache kafka 

2) Apache Spark Streaming

3) Apache Spark Core

3) MongoDB

4) MongoDB Java Connector 

5) Eclipse

6) Maven

7) Cloudera CDH 5.4

A single cluster Kafka queue was created on the server which will be emiiting truckdriver events to the Queue. Following reference was used in this setup
http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/

Then Spark Streaming job was run in Eclipse work space to read events /messages on truckEvent topic and transform it into TruckEvent Java Objects

We used Recieved based approach using KafkaStreamingUtils to integrate with kafka and read the events. 

Dstreams are converted into model objects and persited in mongo db using MongoDB Java Connectors

In total events were processed and persisted in near realtime using 10 seconds duration micro batches

Following Enhancements are planned

1) Use the new Direct Recieved approach to interact with Kafka 

2) Update Apache Cassandra as repository using Saprk SQL

3) Read from the repository and answer following quries , like how many events per driver id ? etc


 


